{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge\n",
    "Do a little scraping or API-calling of your own. Pick a new website and see what you can get out of it. Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.\n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)\n",
    "2) Iterate over multiple pages/queries\n",
    "3) Save the data to your computer\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest. Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-08 20:07:21 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-04-08 20:07:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.4.2, Platform Windows-7-6.1.7601-SP1\n",
      "2019-04-08 20:07:21 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-04-08 20:07:21 [scrapy.extensions.telnet] INFO: Telnet Password: c1f42b6caab6a42e\n",
      "2019-04-08 20:07:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-04-08 20:07:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-04-08 20:07:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-04-08 20:07:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-04-08 20:07:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-04-08 20:07:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-04-08 20:07:22 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2019-04-08 20:07:22 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-04-08 20:07:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 4, 9, 0, 7, 22, 410950),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 8,\n",
      " 'start_time': datetime.datetime(2019, 4, 9, 0, 7, 22, 389949)}\n",
      "2019-04-08 20:07:22 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    name = \"EWSJ\"\n",
    "    \n",
    "    start_url = [\n",
    "        'http://www.wsj.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        with open('wsj_page.html', 'wb') as f:\n",
    "            f.write(response.body)\n",
    "\n",
    "            \n",
    "process = CrawlerProcess()\n",
    "process.crawl(ESSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
