{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def text_cleaner(x):\n",
    "    x = re.sub(r'--', ' ', x)\n",
    "    x = re.sub(r'\\*+', ' ', x)\n",
    "    x = re.sub(\"[\\[].*?[\\]]\", \"\", x)\n",
    "    x = ' '.join(x.split())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_start_time(t = dt.now):\n",
    "    print('start time {}'.format(t))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_end_time(starttime, t = dt.now):\n",
    "    print('end time {}'.format(t))\n",
    "    print('Total time passed is {}'.format(t - starttime))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies(x, include_stop=True):\n",
    "    words = []\n",
    "    for token in x:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_frequencies(text, include_stop=True):\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return Counter(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_words_per_sentence(sentences, df):\n",
    "    words = []\n",
    "    for ss in sentences[0]:\n",
    "        words.append(len(ss)) \n",
    "        \n",
    "    df_local = pd.DataFrame(words, columns=['entity'])\n",
    "    df['count'] = df_local.describe().loc['count', 'entity']\n",
    "    df['25perc'] = df_local.describe().loc['25%', 'entity']\n",
    "    df['50perc'] = df_local.describe().loc['50%', 'entity']\n",
    "    df['75perc'] = df_local.describe().loc['75%', 'entity']\n",
    "    df['max'] = df_local.describe().loc['max', 'entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polysemy_frequencies(x, include_stop=True):\n",
    "    polys = {}\n",
    "    for token in x:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            polys[token] = token.pos_\n",
    "            \n",
    "    return Counter(polys)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_of_polysemy(x, include_stop=True):\n",
    "    container = polysemy_frequencies(x, include_stop)\n",
    "    phrase_types = ['NOUN', 'VERB', 'ADV', 'ADJ', 'ADP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text, nsize, include_stop):\n",
    "    container = lemma_frequencies(text, include_stop).most_common(nsize)\n",
    "    return [item[0] for item in container]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_of_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "    \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Processing row {}'.format(i))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "persuasion_raw = gutenberg.raw('austen-persuasion.txt')\n",
    "alice_raw = gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done CHAPTER\n",
      "Done Cleaner\n"
     ]
    }
   ],
   "source": [
    "# cleanup dataset\n",
    "# remove CHAPTER\n",
    "alice_raw = re.sub(r'CHAPTER .*', '', alice_raw)\n",
    "persuasion_raw = re.sub(r'Chapter \\d+', '', persuasion_raw)\n",
    "print('Done CHAPTER')\n",
    "# make them shorter\n",
    "alice = text_cleaner(alice_raw[:int(len(alice_raw)/10)])\n",
    "persuasion = text_cleaner(persuasion_raw[:int(len(persuasion_raw)/10)])\n",
    "print('Done Cleaner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate features \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenger 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing common_words\n",
    "alice_common = bag_of_words(alice_doc, 30, False)\n",
    "persuasion_common = bag_of_words(persuasion_doc, 30, False)\n",
    "\n",
    "words_common = [x for x in (set(alice_common) - set(persuasion_common))]\n",
    "for y in (set(persuasion_common) - set(alice_common)):\n",
    "    words_common.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n"
     ]
    }
   ],
   "source": [
    "# constructing Dataset\n",
    "features = bow_of_features(sentences, words_common)\n",
    "# add words per sentence statistics\n",
    "stat_words_per_sentence(sentences, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alice</th>\n",
       "      <th>like</th>\n",
       "      <th>wonder</th>\n",
       "      <th>try</th>\n",
       "      <th>begin</th>\n",
       "      <th>oh</th>\n",
       "      <th>foot</th>\n",
       "      <th>door</th>\n",
       "      <th>rabbit</th>\n",
       "      <th>not</th>\n",
       "      <th>...</th>\n",
       "      <th>elliot</th>\n",
       "      <th>sir</th>\n",
       "      <th>'s</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "      <th>count</th>\n",
       "      <th>25perc</th>\n",
       "      <th>50perc</th>\n",
       "      <th>75perc</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>412.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.25</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>412.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.25</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>412.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.25</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  alice like wonder try begin oh foot door rabbit not  ...   elliot sir 's  \\\n",
       "0     2    0      0   0     1  0    0    0      0   0  ...        0   0  0   \n",
       "1     0    0      0   0     0  0    0    0      1   0  ...        0   0  0   \n",
       "2     1    0      0   0     0  1    0    0      1   0  ...        0   0  0   \n",
       "\n",
       "                                       text_sentence text_source  count  \\\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  412.0   \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  412.0   \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  412.0   \n",
       "\n",
       "  25perc 50perc 75perc    max  \n",
       "0   12.0   23.0  42.25  204.0  \n",
       "1   12.0   23.0  42.25  204.0  \n",
       "2   12.0   23.0  42.25  204.0  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = features['text_source']\n",
    "X = features.drop(['text_source', 'text_sentence'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.9081081081081082\n",
      "Test set score : 0.8986784140969163\n"
     ]
    }
   ],
   "source": [
    "# RandomForestModel\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.55,\n",
    "                                                    random_state=4)\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(rfc.score(X_train, Y_train)))\n",
    "print('Test set score : {}'.format(rfc.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.8918918918918919\n",
      "Test set score : 0.9074889867841409\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(lr.score(X_train, Y_train)))\n",
    "print('Test set score : {}'.format(lr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.9081081081081082\n",
      "Test set score : 0.8942731277533039\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Model\n",
    "gbc = ensemble.GradientBoostingClassifier()\n",
    "gbc.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(gbc.score(X_train, Y_train)))\n",
    "print('Test set score : {}'.format(gbc.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 'milton-paradise.txt'\n",
    "paradise_raw = gutenberg.raw('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Book\n",
      "Done Cleaner\n"
     ]
    }
   ],
   "source": [
    "# cleanup dataset\n",
    "# remove Book\n",
    "paradise_raw = re.sub(r'Book [A-Z]*', '', paradise_raw)\n",
    "print('Done Book')\n",
    "# make them shorter\n",
    "paradise = text_cleaner(paradise_raw[:int(len(alice_raw)/2)])\n",
    "print('Done Cleaner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nlp = spacy.load('en_core_web_sm')\n",
    "paradise_doc = nlp(paradise)\n",
    "\n",
    "paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "\n",
    "sentences = pd.DataFrame(alice_sents + paradise_sents)\n",
    "sentences.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing common_words\n",
    "alice_common = bag_of_words(alice_doc, 30, False)\n",
    "paradise_common = bag_of_words(paradise_doc, 30, False)\n",
    "\n",
    "words_common = [x for x in (set(alice_common) - set(paradise_common))]\n",
    "for y in (set(paradise_common) - set(alice_common)):\n",
    "    words_common.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n"
     ]
    }
   ],
   "source": [
    "# constructing Dataset\n",
    "features = bow_of_features(sentences, words_common)\n",
    "# add words per sentence statistics\n",
    "stat_words_per_sentence(sentences, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = features['text_source']\n",
    "X = features.drop(['text_source', 'text_sentence'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.9488188976377953\n",
      "Test set score : 0.907051282051282\n"
     ]
    }
   ],
   "source": [
    "# RandomForestModel\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.55,\n",
    "                                                    random_state=4)\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(rfc.score(X_train, Y_train)))\n",
    "print('Test set score : {}'.format(rfc.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.9094488188976378\n",
      "Test set score : 0.8974358974358975\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(lr.score(X_train, Y_train)))\n",
    "print('Test set score : {}'.format(lr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.9409448818897638\n",
      "Test set score : 0.9038461538461539\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Model\n",
    "gbc = ensemble.GradientBoostingClassifier()\n",
    "gbc.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(gbc.score(X_train, Y_train)))\n",
    "print('Test set score : {}'.format(gbc.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
