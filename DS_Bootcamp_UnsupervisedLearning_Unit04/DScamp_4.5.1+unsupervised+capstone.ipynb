{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Capstone\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.corpus import brown, crubadan, timit\n",
    "\n",
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# clustering models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "def text_cleaner(x):\n",
    "    \"\"\"Removing certain chars by using regular expressions.\"\"\"\n",
    "    x = re.sub(r'--', ' ', x)\n",
    "    x = re.sub(r'\\*+', ' ', x)\n",
    "    x = re.sub(\"[\\[].*?[\\]]\", \"\", x)\n",
    "    x = ' '.join(x.split())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_start_time(t = dt.now):\n",
    "    \"\"\"A system timer for start time.\"\"\"\n",
    "    print('start time {}'.format(t))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_end_time(starttime, t = dt.now):\n",
    "    \"\"\"A system timer for end time.\"\"\"\n",
    "    print('end time {}'.format(t))\n",
    "    print('Total time passed is {}'.format(t - starttime))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies(x, include_stop=True):\n",
    "    \"\"\"Generate word counts for a string.\"\"\"\n",
    "    words = []\n",
    "    for token in x:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \"\"\"Generate word lemma counts for a string\"\"\"\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return Counter(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text, nsize, include_stop):\n",
    "    \"\"\"Generate list of words contained in a string.\"\"\"\n",
    "    container = lemma_frequencies(text, include_stop).most_common(nsize)\n",
    "    return [item[0] for item in container]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_of_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "    \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('Processing row {}'.format(i))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_words_per_sentence(sentences, df):\n",
    "    words = []\n",
    "    for ss in sentences[0]:\n",
    "        words.append(len(ss)) \n",
    "        \n",
    "    df_local = pd.DataFrame(words, columns=['entity'])\n",
    "    df['count'] = df_local.describe().loc['count', 'entity']\n",
    "    df['25perc'] = df_local.describe().loc['25%', 'entity']\n",
    "    df['50perc'] = df_local.describe().loc['50%', 'entity']\n",
    "    df['75perc'] = df_local.describe().loc['75%', 'entity']\n",
    "    df['max'] = df_local.describe().loc['max', 'entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#print(brown.fileids())\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "austen1 = gutenberg.raw('austen-' + 'emma' + '.txt')\n",
    "austen2 = gutenberg.raw('austen-' + 'persuasion' + '.txt')\n",
    "bryant = gutenberg.raw('bryant-' + 'stories' + '.txt')\n",
    "burgess = gutenberg.raw('burgess-busterbrown' + '.txt')\n",
    "carroll = gutenberg.raw('carroll-alice' + '.txt')\n",
    "chesterton = gutenberg.raw('chesterton-brown' + '.txt')\n",
    "edgeworth = gutenberg.raw('edgeworth-parents' + '.txt')\n",
    "melville = gutenberg.raw('melville-moby_dick' + '.txt')\n",
    "milton = gutenberg.raw('milton-paradise' + '.txt')\n",
    "shakespeare = gutenberg.raw('shakespeare-caesar' + '.txt')\n",
    "whitman = gutenberg.raw('whitman-leaves' + '.txt')\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup dataset\n",
    "# remove VOLUME, CHAPTER\n",
    "austen1 = re.sub(r'CHAPTER [I]*[V]*[X]*', '', austen1)\n",
    "austen1 = re.sub(r'VOLUME [I]*[V]*[X]*', '', austen1)\n",
    "austen1 = text_cleaner(austen1)\n",
    "                 \n",
    "austen2 = re.sub(r'Chapter \\d+', '', austen2)\n",
    "austen2 = text_cleaner(austen2)\n",
    "                 \n",
    "bryant = text_cleaner(bryant)\n",
    "                 \n",
    "burgess = re.sub(r'[I]*[V]*[X]*\\n', '', burgess)\n",
    "burgess = text_cleaner(burgess)\n",
    "\n",
    "#carroll = re.sub[r'CHAPTER [A-Z]*.', '', carroll]\n",
    "carroll = text_cleaner(carroll)                \n",
    "                 \n",
    "chesterton = text_cleaner(chesterton)\n",
    "\n",
    "edgeworth = text_cleaner(edgeworth)\n",
    "                 \n",
    "milton = text_cleaner(milton)\n",
    "\n",
    "shakespeare = text_cleaner(shakespeare)\n",
    "\n",
    "whitman = text_cleaner(whitman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# feature engineer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "austen1_doc = nlp(austen1)\n",
    "austen2_doc = nlp(austen2)\n",
    "bryant_doc = nlp(bryant)\n",
    "burgess_doc = nlp(burgess)\n",
    "carroll_doc = nlp(carroll)\n",
    "chesterton_doc = nlp(chesterton)\n",
    "edgeworth_doc = nlp(edgeworth)\n",
    "milton_doc = nlp(milton)\n",
    "shakespeare_doc = nlp(shakespeare)\n",
    "whitman_doc = nlp(whitman)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Emma, Woodhouse, ,, handsome, ,, clever, ,, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  (Emma, Woodhouse, ,, handsome, ,, clever, ,, a...  0\n",
       "1  (She, was, the, youngest, of, the, two, daught...  0\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  0\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  0\n",
       "4                                 (Between, _, them)  0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group into sentences\n",
    "austen1_sents = [[sent, 0] for sent in austen1_doc.sents]                       # austen1 ---  0\n",
    "austen2_sents = [[sent, 1] for sent in austen2_doc.sents]                       # austen2 ---  1\n",
    "bryant_sents = [[sent, 2] for sent in bryant_doc.sents]                         # bryant  ---  2\n",
    "burgess_sents = [[sent, 3] for sent in burgess_doc.sents]                       # burgess ---  3\n",
    "carroll_sents = [[sent, 4] for sent in carroll_doc.sents]                       # carroll ---  4\n",
    "chesterton_sents = [[sent, 5] for sent in chesterton_doc.sents]                 # chesterton ---  5\n",
    "edgeworth_sents = [[sent, 6] for sent in edgeworth_doc.sents]                   # edgeworth ---6\n",
    "milton_sents = [[sent, 7] for sent in milton_doc.sents]                         # milton  ---  7    \n",
    "shakespeare_sents = [[sent, 8] for sent in shakespeare_doc.sents]               # shakespeare --- 8\n",
    "whitman_sents = [[sent, 9] for sent in whitman_doc.sents]                       # whitman ---  9\n",
    "\n",
    "sentences = pd.DataFrame(austen1_sents +\n",
    "                         austen2_sents +\n",
    "                         bryant_sents +\n",
    "                         burgess_sents +\n",
    "                         carroll_sents +\n",
    "                         chesterton_sents +\n",
    "                         edgeworth_sents +\n",
    "                         milton_sents +\n",
    "                         shakespeare_sents +\n",
    "                         whitman_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing bag_of_words\n",
    "austen1_common = bag_of_words(austen1_doc, 20, False)        \n",
    "austen2_common = bag_of_words(austen2_doc, 20, False)\n",
    "bryant_common = bag_of_words(bryant_doc, 20, False)\n",
    "burgess_common = bag_of_words(burgess_doc, 20, False)\n",
    "carroll_common = bag_of_words(carroll_doc, 20, False)\n",
    "chesterton_common = bag_of_words(chesterton_doc, 20, False)\n",
    "edgeworth_common = bag_of_words(edgeworth_doc, 20, False)\n",
    "milton_common = bag_of_words(milton_doc, 20, False)\n",
    "shakespeare_common = bag_of_words(shakespeare_doc, 20, False)\n",
    "whitman_common = bag_of_words(whitman_doc, 20, False)\n",
    "\n",
    "# common and unique words\n",
    "words_common = [x for x in (set(austen1_common) - set(austen2_common) - set(bryant_common) - set(burgess_common) -\n",
    "                            set(carroll_common) - set(chesterton_common) - set(edgeworth_common) - set(milton_common) -\n",
    "                            set(shakespeare_common) - set(whitman_common))]\n",
    "\n",
    "for y in (set(austen2_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(milton_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(bryant_common) - set(austen1_common) - set(austen2_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(milton_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(burgess_common) - set(austen1_common) - set(bryant_common) - set(austen2_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(milton_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(carroll_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(austen2_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(milton_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(chesterton_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(austen2_common) - set(edgeworth_common) - set(milton_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(edgeworth_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(austen2_common) - set(milton_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(milton_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(austen2_common) - set(shakespeare_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(shakespeare_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(milton_common) - set(austen2_common) - set(whitman_common)):\n",
    "    words_common.append(y)\n",
    "    \n",
    "for y in (set(whitman_common) - set(austen1_common) - set(bryant_common) - set(burgess_common) - set(carroll_common) - \n",
    "          set(chesterton_common) - set(edgeworth_common) - set(milton_common) - set(shakespeare_common) - set(austen2_common)):\n",
    "    words_common.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 1000\n",
      "Processing row 2000\n",
      "Processing row 3000\n",
      "Processing row 4000\n",
      "Processing row 5000\n",
      "Processing row 6000\n",
      "Processing row 7000\n",
      "Processing row 8000\n",
      "Processing row 9000\n",
      "Processing row 10000\n",
      "Processing row 11000\n",
      "Processing row 12000\n",
      "Processing row 13000\n",
      "Processing row 14000\n",
      "Processing row 15000\n",
      "Processing row 16000\n",
      "Processing row 17000\n",
      "Processing row 18000\n",
      "Processing row 19000\n",
      "Processing row 20000\n",
      "Processing row 21000\n",
      "Processing row 22000\n",
      "Processing row 23000\n",
      "Processing row 24000\n",
      "Processing row 25000\n",
      "Processing row 26000\n",
      "Processing row 27000\n",
      "Processing row 28000\n",
      "Processing row 29000\n",
      "Processing row 30000\n",
      "Processing row 31000\n",
      "Processing row 32000\n",
      "Processing row 33000\n",
      "Processing row 34000\n",
      "Processing row 35000\n",
      "Processing row 36000\n",
      "Processing row 37000\n",
      "Processing row 38000\n",
      "Processing row 39000\n",
      "Processing row 40000\n",
      "Processing row 41000\n",
      "Processing row 42000\n"
     ]
    }
   ],
   "source": [
    "# constructing Dataset\n",
    "features = bow_of_features(sentences, words_common)\n",
    "# add words per sentence statistics\n",
    "stat_words_per_sentence(sentences, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now start with clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start clustering models\n",
    "Y = features['text_source']\n",
    "X = features.drop(['text_source', 'text_sentence'], 1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions: \n",
      "text_source     0     1     2    3     4     5     6     7     8     9\n",
      "row_0                                                                 \n",
      "0             454   112     1    0     5    32   169     4     0     5\n",
      "1            6969  3009  2387  863  1510  3082  8500  1963  2057  4865\n",
      "2             201   106    69   27    48    96   301    64     9   143\n",
      "3             130    83    50   19    31    82   116    70     6   180\n",
      "4             155    96    65    0     5   205   231    44     1    31\n",
      "5              10     5    14   14    12     6    14   454    17    88\n",
      "6             150    58    54   27    37    92   288    21    40    51\n",
      "7             111    48    21    2     2     6    86    74     0   241\n",
      "8             182   132   104   48    28   115   238    69    14   105\n",
      "9             561     0     0    1     0     0   224     0     0     0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KMeans cluster model\n",
    "X_norm = normalize(X)\n",
    "X_pca = PCA(10).fit_transform(X_norm)\n",
    "Y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X_pca)\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=Y_pred)\n",
    "\n",
    "# Check the prediction against the data\n",
    "print('Comparing k-means and mini batch k-means solutions: ')\n",
    "print(pd.crosstab(Y_pred, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like the KMeans model don't do very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions: \n",
      "col_0    0      1    2    3    4    5    6    7    8    9\n",
      "row_0                                                    \n",
      "0        0     21  892    0    0    0  722    4    0    0\n",
      "1        7      0   28  766    1    4   20    9   26    0\n",
      "2        2    131   17    0    0  628    8   92   14    0\n",
      "3      729    592   83    0    0    1   18   32  956    0\n",
      "4        0   2240    0    0    0    0    0    0    0    0\n",
      "5       36      0   21    0    0    0   18    3    9  784\n",
      "6        7      0   23    1  832    1   32    7   30    0\n",
      "7        0  28979    0    0    0    0    0    0    0    0\n",
      "8        1   1509    0    0    0    0    0    0    0    2\n",
      "9        0   1733    0    0    0    0    0  444    0    0\n"
     ]
    }
   ],
   "source": [
    "# use miniBatch\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=10,\n",
    "    batch_size=300)\n",
    "minibatchkmeans.fit(X_pca)\n",
    "pred_mini = minibatchkmeans.predict(X_pca)\n",
    "\n",
    "# Check the prediction against the previous solution\n",
    "print('Comparing k-means and mini batch k-means solutions: ')\n",
    "print(pd.crosstab(pred_mini, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing mini batch k-means solutions and data: \n",
      "text_source     0     1     2    3     4     5     6     7     8     9\n",
      "row_0                                                                 \n",
      "0             292   141   115   54    78   166   514    58    49   172\n",
      "1             146    97    54   19    35    95   130    76     6   203\n",
      "2              13     7    21   16    14     8    19   606    18   170\n",
      "3             615   240   111   49    37   137   440   125    82   575\n",
      "4             185   123   242   25   107   151   262   563    92   490\n",
      "5             606     0     0    1     0     0   264     0     0     0\n",
      "6             173   112    73    0     5   224   263    47     2    34\n",
      "7            5258  2251  2004  675  1347  2845  7828  1180  1842  3749\n",
      "8            1124   158    43   11    37    36    67    19     5    12\n",
      "9             511   520   102  151    18    54   380    89    48   304\n"
     ]
    }
   ],
   "source": [
    "print('Comparing mini batch k-means solutions and data: ')\n",
    "print(pd.crosstab(pred_mini, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters : 958\n"
     ]
    }
   ],
   "source": [
    "# use MeanShift model\n",
    "bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=300)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X_train)\n",
    "\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "print(\"Number of estimated clusters : {}\".format(n_clusters_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score : 0.24770749309346918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.20400486, 0.20804485, 0.20679085, 0.19965974, 0.21713367])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = ensemble.RandomForestRegressor()\n",
    "rfr.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score : {}'.format(rfr.score(X_train, Y_train)))\n",
    "cross_val_score(rfr, X_train, Y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set score : 0.20963985578656152\n"
     ]
    }
   ],
   "source": [
    "# Now take it to test dataset\n",
    "print('Testing set score : {}'.format(rfr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set score : {}'.format(rfr.score(X_test, Y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
